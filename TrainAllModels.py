
# coding: utf-8

# In[1]:

import tensorflow as tf
import keras
from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Activation
from keras.models import Sequential
from keras.callbacks import TensorBoard
from keras.optimizers import SGD
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
import h5py
import numpy as np
import pickle
import glob
import os
import matplotlib.pyplot as plt
import numpy as np
import models
from keras.utils import np_utils


# In[2]:

batch_size = 56
nb_classes = 10
nb_epoch = 100
data_augmentation = False
ITERATIONS = 10


# In[3]:

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
print('X_train shape:', x_train.shape)
print('y_train shape:', y_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = np_utils.to_categorical(y_train, nb_classes)
y_test = np_utils.to_categorical(y_test, nb_classes)
print('y_train shape:', y_train.shape)


# In[4]:

#Data normalization
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255


# In[5]:

models_to_train = [models.build_model_1(), models.build_model_2(), models.build_model_3(), models.build_model_4()]
for i in range(len(models_to_train)):
    models_to_train[i].compile(optimizer=SGD(momentum=0.9, lr=0.001), metrics=['accuracy'], loss='categorical_crossentropy')


# In[5]:


# In[8]:

def train_and_save_model(model, data_augmentation, modelname):
    if not data_augmentation:
        print('Not using data augmentation.')
        
        for i in range(ITERATIONS):
            print("[DEBUG] Iteration {0}".format(i))
            history = model.fit(x_train,y_train,
                      batch_size=batch_size,
                      epochs=nb_epoch // ITERATIONS,
                      validation_split=0.3,
                      shuffle=True,
                      verbose=1,
                      callbacks=[TensorBoard(log_dir='./log_' + modelname, histogram_freq=0,write_graph=True,write_images=True,embeddings_freq=4)])
            print("[DEBUG] Saving model")
            model.save(modelname + '.h5')

    else:
        print('Using real-time data augmentation.')

        # this will do preprocessing and realtime data augmentation
        datagen = ImageDataGenerator(
            featurewise_center=False,  # set input mean to 0 over the dataset
            samplewise_center=False,  # set each sample mean to 0
            featurewise_std_normalization=False,  # divide inputs by std of the dataset
            samplewise_std_normalization=False,  # divide each input by its std
            zca_whitening=False,  # apply ZCA whitening
            rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)
            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
            horizontal_flip=True,  # randomly flip images
            vertical_flip=False)  # randomly flip images

        # compute quantities required for featurewise normalization
        # (std, mean, and principal components if ZCA whitening is applied)
        #Only required if featurewise_center or featurewise_std_normalization or zca_whitening
        datagen.fit(x_train)
		
        
        for i in range(ITERATIONS):
            print ("[DEBUG] Iteration {0}".format(i))
            # fit the model on the batches generated by datagen.flow()
            history = model.fit_generator(datagen.flow(x_train, y_train,
                            batch_size=batch_size),
                            steps_per_epoch=len(x_train)//batch_size, #it should be equal to x_train.shape[0]/batch_size
                            epochs=nb_epoch // ITERATIONS,
							verbose=1,
                            callbacks=[TensorBoard(log_dir='./logGen' + modelname, histogram_freq=4,write_graph=True,write_images=True,embeddings_freq=4)])
            print("[DEBUG] Saving model")
            model.save(modelname + '.h5')

    model.save(modelname + '.h5')


# In[9]:

for i in range(len(models_to_train)):
    try:
        train_and_save_model(models_to_train[i], True, 'model_{0}_with_random'.format(i))
    except Exception as e:
        print("[ERROR] Could not complete training of model {0} with random".format(i))
        print(e)
    
	
    try:
        train_and_save_model(models_to_train[i], False, 'model_{0}_withoud_random'.format(i))
    except Exception as e:
        print("[ERROR] Could not complete training of model {0} with random".format(i))
        print(e)



