{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_file(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_train(path):\n",
    "    train_data = []\n",
    "    labels = []\n",
    "    for file in glob.glob(os.path.join(path, 'data_batch*')):\n",
    "        data = read_from_file(file)\n",
    "        labels.append(data[b'labels'])\n",
    "        train_data.append(data[b'data'])\n",
    "    \n",
    "    train_data = np.concatenate(train_data)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    return train_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = read_train('Cifar10')\n",
    "y_labels = np.eye(10)[y_train]\n",
    "x_train = x_train.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def split_in_batches(data, batch_size):\n",
    "    ret = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    num_batches = math.ceil(data.shape[0] / batch_size)\n",
    "    for batch_num in range(num_batches):\n",
    "        end += batch_size\n",
    "        if end > data.shape[0]:  #last batch\n",
    "            ret.append(data[-batch_size:])\n",
    "        else:\n",
    "            ret.append(data[start:end])\n",
    "        start = end\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = split_in_batches(x_train, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the two models from a previously trained model. \n",
    "\n",
    "Note that this is the only usage of Keras (for building and training the net). Keras uses tensors to build its net, so once we get the last tensor there would be no difference between keras and a pure tensorflow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model_gpu = load_model('model1.h5')\n",
    "    x_gpu = tf.placeholder(tf.float32, shape=(None, 32,32,3))\n",
    "with tf.device('/cpu:0'):\n",
    "    model_cpu = load_model('model1.h5')\n",
    "    x_cpu = tf.placeholder(tf.float32, shape=(None, 32,32,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same example as defined in the tensorflow tutorial but placing one CPU and one GPU instead of two GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('model1.h5')\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_2 = []   # Concat the result of the two tensor but with the same input tensor\n",
    "for dev in ['/gpu:0', '/cpu:0']:\n",
    "    with tf.device(dev):\n",
    "        c_2.append(model(x))\n",
    "with tf.device('/cpu:0'):\n",
    "    predictions_2 = tf.concat(c_2, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought that maybe one bottleneck was the placement of the placeholder. So we also tried to instantiate two different placeholders for the two models, each of them is declared in CPU and GPU respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = []   # Concat the result of the two tensors with diffent input tensor\n",
    "with tf.device('/gpu:0'):\n",
    "    c.append(model_gpu(x_gpu))\n",
    "with tf.device('/cpu:0'):\n",
    "    c.append(model_cpu(x_cpu))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    predictions = tf.concat(c, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "session = tf.Session(config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the tensorflow tutorial, this tensor (predictions_2) should automatically split the workload across both the devices, if the devices were both gpu cards. Here we put one GPU and one CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21 s ± 71.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "for b in batches:\n",
    "    session.run(predictions_2, feed_dict = {x : b})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the workload was divided equally between CPU and GPU, I would expect CPU usage to be 100%. Instead for this run it is fixed at about 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we wanted to try out our model with the different placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.2 ms ± 5.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Run all on the GPU\n",
    "%timeit -n 10 ret = session.run(predictions, feed_dict={x_cpu : x_train[:1], x_gpu : x_train[1:1024]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276 ms ± 3.78 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Run 50% on CPU and 50% on GPU\n",
    "%timeit -n 10 ret = session.run(predictions, feed_dict={x_cpu : x_train[:512], x_gpu : x_train[512:1024]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558 ms ± 6.74 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Run 100% on CPU\n",
    "%timeit -n 10 ret = session.run(predictions, feed_dict={x_cpu : x_train[1:1024], x_gpu : x_train[:1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.7 ms ± 9.89 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Run most on GPU, few instances on CPU\n",
    "%timeit -n 10 ret = session.run(predictions, feed_dict={x_cpu : x_train[:2], x_gpu : x_train[2:1024]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the time of the last run, with only one more batch for CPU, I would expect it to be less than the time of using only the GPU (here the GPU is clearly the bottleneck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using thread alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_on_device(session, predict_tensor, batches):\n",
    "    #session = tf.Session()\n",
    "    #session.run(tf.global_variables_initializer())\n",
    "    for batch in batches:\n",
    "        session.run(predict_tensor, feed_dict={x : batch})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_cpu_gpu(batches, num_batches_cpu, tensor_cpu, tensor_gpu):\n",
    "    session = tf.Session(config=tf.ConfigProto(log_device_placement=True, intra_op_parallelism_threads=8))\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    \n",
    "    threads = []\n",
    "    #threads += [Thread(target=predict_on_device, args=(session, tensor_cpu, batches))]\n",
    "    threads += [Thread(target=predict_on_device, args=(session, tensor_gpu, batches))]\n",
    "    \n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    \n",
    "    coord.join(threads)\n",
    "    \n",
    "    session.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload all the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 32,32,3))\n",
    "    model_gpu = load_model('model1.h5')\n",
    "    tensor_gpu = model_gpu(x)\n",
    "    \n",
    "with tf.device('/cpu:0'):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 32,32,3))\n",
    "    model_cpu = load_model('model1.h5')\n",
    "    tensor_cpu = model_cpu(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all the batches, first only on GPU, then 20 on CPU and other in GPU and finally 40 on CPU and the other in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-42:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1022, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1004, in _run_fn\n",
      "    status, run_metadata)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\contextlib.py\", line 66, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n",
      "    pywrap_tensorflow.TF_GetCode(status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_285' with dtype float\n",
      "\t [[Node: Placeholder_285 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "\t [[Node: sequential_14/dense_2/Softmax/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_30_sequential_14/dense_2/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-43-0423ff3845d4>\", line 5, in predict_on_device\n",
      "    session.run(predict_tensor, feed_dict={x : batch})\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 767, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 965, in _run\n",
      "    feed_dict_string, options, run_metadata)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1015, in _do_run\n",
      "    target_list, options, run_metadata)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1035, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_285' with dtype float\n",
      "\t [[Node: Placeholder_285 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "\t [[Node: sequential_14/dense_2/Softmax/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_30_sequential_14/dense_2/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
      "\n",
      "Caused by op 'Placeholder_285', defined at:\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-42-bda4c4e90e66>\", line 3, in <module>\n",
      "    x = tf.placeholder(tf.float32, shape=(None, 32,32,3))\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1502, in placeholder\n",
      "    name=name)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2149, in _placeholder\n",
      "    name=name)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"c:\\users\\andre\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_285' with dtype float\n",
      "\t [[Node: Placeholder_285 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "\t [[Node: sequential_14/dense_2/Softmax/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_30_sequential_14/dense_2/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "split_cpu_gpu(batches[:10], 0, tensor_cpu, tensor_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 s ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -c 10\n",
    "split_cpu_gpu(batches, 20, tensor_cpu, tensor_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.2 s ± 33.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -c 10\n",
    "split_cpu_gpu(batches, 40, tensor_cpu, tensor_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that this 'thread' thing takes up much more time than the one did before (around 2 sec). I honestly can't explain this difference (maybe the overhead for creating the thread is relevant, but it doesn't explain the whole 2 seconds delay, because there shouldn't be any conflicts among the two threads (all the variables are different)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction only on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    pred_only_gpu = model_gpu(x_gpu)\n",
    "session = tf.Session(config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 s ± 7.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "for b in batches:\n",
    "    session.run(pred_only_gpu, feed_dict = {x_gpu : b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the previous results, running the prediction on only the GPU takes only 2.2 seconds, with is less than all the other possibilities we have tried so far.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
